{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.cluster import BisectingKMeans\n",
    "\n",
    "from helpers.utils import get_metrics, set_matplotlib_params\n",
    "from networks.nonlinearnet_aihuman import NonLinearNetDefer, optimization_loop\n",
    "\n",
    "set_matplotlib_params()\n",
    "warnings.filterwarnings('ignore')\n",
    "seed = 12\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(seed)\n",
    "rng = np.random.default_rng(seed) \n",
    "torch.set_default_dtype(torch.double)\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (21302, 2052)\n",
      "Train undersampled size: (2420, 2053)\n",
      "Train undersampled human size: (2420, 2053)\n"
     ]
    }
   ],
   "source": [
    "drd2_train = pd.read_csv(\"datasets/drd2_train_ECFP_counts.csv\")\n",
    "drd2_train_undersampled = pd.read_csv(\"datasets/drd2_train_undersampled_ECFP_counts.csv\")\n",
    "drd2_test = pd.read_csv(\"datasets/drd2_test_ECFP_counts.csv\")\n",
    "CLUSTERING = True\n",
    "\n",
    "# keeping some of the samples to create the AL pool. npool is the number of samples we REMOVE from the training set\n",
    "# setting it to 0, as you get additional human samples from Reinvent.\n",
    "npool = 0\n",
    "idxpool = rng.choice(range(len(drd2_train_undersampled)), npool, replace=False)\n",
    "pool = drd2_train_undersampled.iloc[idxpool]\n",
    "drd2_train_undersampled.drop(idxpool, axis=0, inplace=True)\n",
    "d = 2048\n",
    "\n",
    "npts_more_human = 0\n",
    "keep_pool = np.random.choice(pool.index, npts_more_human, replace=False)\n",
    "drd2_train_undersampled_h = pd.concat((drd2_train_undersampled, pool.loc[keep_pool]))\n",
    "\n",
    "print(f\"Train size: {drd2_train.shape}\")\n",
    "print(f\"Train undersampled size: {drd2_train_undersampled.shape}\")\n",
    "print(f\"Train undersampled human size: {drd2_train_undersampled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "drd2_train_undersampled[\"activity_y\"] = drd2_train_undersampled.activity.values.tolist()\n",
    "drd2_train_undersampled_h[\"activity_h\"] = drd2_train_undersampled_h.activity.values.tolist()\n",
    "\n",
    "drd2_test[\"activity_y\"] = drd2_test.activity.values.tolist()\n",
    "drd2_test[\"activity_h\"] = drd2_test.activity.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size: torch.Size([930, 2048])\n"
     ]
    }
   ],
   "source": [
    "train_features = drd2_train_undersampled[[f\"bit{i}\" for i in range(d)]].values\n",
    "train_features_h = drd2_train_undersampled_h[[f\"bit{i}\" for i in range(d)]].values\n",
    "train_labels = drd2_train_undersampled[[\"activity_y\"]].values\n",
    "train_labels_h = drd2_train_undersampled_h[[\"activity_h\"]].values\n",
    "\n",
    "test_features = drd2_test[[f\"bit{i}\" for i in range(d)]].values\n",
    "test_labels = drd2_test[[\"activity_y\", \"activity_h\"]].values\n",
    "\n",
    "X_train = torch.tensor(train_features, dtype=torch.double)\n",
    "X_train_h = torch.tensor(train_features_h, dtype=torch.double)\n",
    "y_train = torch.tensor(train_labels, dtype=torch.double)\n",
    "h_train = torch.tensor(train_labels_h, dtype=torch.double)\n",
    "\n",
    "X_test = torch.tensor(test_features, dtype=torch.double)\n",
    "y_test = torch.tensor(test_labels, dtype=torch.double)\n",
    "\n",
    "idx_active = torch.where(y_test[:, 0])[0].numpy()\n",
    "idx_inactive = [i for i in range(len(y_test)) if i not in idx_active]\n",
    "idx_inactive = np.random.choice(range(len(y_test)), 600, replace=False)\n",
    "idx = np.r_[idx_active,idx_inactive]\n",
    "y_test = y_test[idx]\n",
    "X_test = X_test[idx]\n",
    "print(f\"Test size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example usage:\n",
    "num_features = train_features.shape[1]  # number of input features\n",
    "num_epochs = 100\n",
    "lr = 0.1\n",
    "\n",
    "# create an instance of the NonLinearNetDefer\n",
    "l2d_model = NonLinearNetDefer(num_features)\n",
    "\n",
    "# define the loss function and optimizer for the l2d_model\n",
    "criterion = nn.BCEWithLogitsLoss()  # use BCEWithLogitsLoss for binary classification\n",
    "optimizer = optim.SGD(l2d_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train\n",
    "X_h = X_train_h\n",
    "\n",
    "# binary labels for classifier 1 and classifier 2 (or human model)\n",
    "y_train = y_train[:,0].unsqueeze(1)\n",
    "h_train = h_train[:,0].unsqueeze(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLUSTERING:\n",
    "    algo = BisectingKMeans(n_clusters=2, random_state=seed, n_init=20, max_iter=5000, init=\"k-means++\")\n",
    "    algo.fit(X)\n",
    "    c1 = np.where(algo.labels_)[0]\n",
    "    c2 = [i for i in range(len(X_train)) if i not in c1]\n",
    "\n",
    "    X_train = X_train[c1, :]\n",
    "    X_train_h = X_train_h[c2,:]\n",
    "\n",
    "    y_train = y_train[c1,0].unsqueeze(1)\n",
    "    h_train = h_train[c2,0].unsqueeze(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add random noise to h\n",
    "# oldh = torch.clone(h)\n",
    "# p =torch.bernoulli(0.75 *torch.ones(len(h))).unsqueeze(1)\n",
    "# h = p*h + (1-p)*(1-h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 2.718007993964455\n",
      "Epoch [20/100], Loss: 2.551324379572577\n",
      "Epoch [30/100], Loss: 2.422772061085868\n",
      "Epoch [40/100], Loss: 2.349123355218382\n",
      "Epoch [50/100], Loss: 2.35394383481985\n",
      "Epoch [60/100], Loss: 2.2697388083584205\n",
      "Epoch [70/100], Loss: 2.2420745059762344\n",
      "Epoch [80/100], Loss: 2.2232911348515234\n",
      "Epoch [90/100], Loss: 2.2079091210173614\n",
      "Epoch [100/100], Loss: 2.1958071373028334\n"
     ]
    }
   ],
   "source": [
    "optimization_loop(num_epochs, optimizer, l2d_model, X_train, X_train_h, y_train, h_train, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"clf_1\": {\n",
      "    \"Accuracy\": 0.9451612903225807,\n",
      "    \"Precision\": 0.9514069557277025,\n",
      "    \"Recall\": 0.9451612903225807,\n",
      "    \"F1-Score\": 0.9456364035235633\n",
      "  },\n",
      "  \"clf_2\": {\n",
      "    \"Accuracy\": 0.8591397849462366,\n",
      "    \"Precision\": 0.8850645523923252,\n",
      "    \"Recall\": 0.8591397849462366,\n",
      "    \"F1-Score\": 0.8607409120680708\n",
      "  },\n",
      "  \"system\": {\n",
      "    \"Accuracy\": 0.9451612903225807,\n",
      "    \"Precision\": 0.9514069557277025,\n",
      "    \"Recall\": 0.9451612903225807,\n",
      "    \"F1-Score\": 0.9456364035235633\n",
      "  }\n",
      "}\n",
      "Percentage of deferral: 0.30215054750442505\n"
     ]
    }
   ],
   "source": [
    "y_test = y_test[:,0].unsqueeze(1)\n",
    "h_test = y_test[:,0].unsqueeze(1) # y = h\n",
    "metrics = {}\n",
    "labels = [y_test, h_test]\n",
    "\n",
    "with torch.no_grad():\n",
    "    l2d_model.eval()\n",
    "    combined_outputs, decision_outputs = l2d_model(X_test)\n",
    "    pred_clf = (combined_outputs > 0.5).float()\n",
    "    for i in range(2):\n",
    "        metrics[f\"clf_{i+1}\"] = get_metrics(labels[i], pred_clf[:, i])\n",
    "    boolean = (decision_outputs[:, -1] > decision_outputs[:, 0]) * (decision_outputs[:, 1] > decision_outputs[:, 0]) * 1.\n",
    "    boolean = torch.tensor(boolean, dtype=torch.float32)\n",
    "\n",
    "    final_predictions = (boolean * pred_clf[:, 1]) + (1 - boolean) * pred_clf[:, 0]\n",
    "    metrics[f\"system\"] = get_metrics(labels[0], final_predictions)\n",
    "print (json.dumps(metrics, indent=2, default=str))\n",
    "print(f\"Percentage of deferral: {boolean.mean()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
