{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from helpers.utils import get_metrics, set_matplotlib_params\n",
    "from networks.nonlinearnet_aihuman import NonLinearNetDefer, optimization_loop\n",
    "\n",
    "set_matplotlib_params()\n",
    "warnings.filterwarnings('ignore')\n",
    "seed = 10\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(seed)\n",
    "rng = np.random.default_rng(seed) \n",
    "torch.set_default_dtype(torch.double)\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (21302, 2052)\n",
      "Train undersampled size: (1420, 2053)\n",
      "Train undersampled human size: (1420, 2053)\n"
     ]
    }
   ],
   "source": [
    "drd2_train = pd.read_csv(\"datasets/drd2_train_ECFP_counts.csv\")\n",
    "drd2_train_undersampled = pd.read_csv(\"datasets/drd2_train_undersampled_ECFP_counts.csv\")\n",
    "drd2_test = pd.read_csv(\"datasets/drd2_test_ECFP_counts.csv\")\n",
    "CLUSTERING = True\n",
    "\n",
    "# keeping some of the samples to create the AL pool. npool is the number of samples we REMOVE from the training set\n",
    "# setting it to 0, as you get additional human samples from Reinvent.\n",
    "npool = 1000\n",
    "idxpool = rng.choice(range(len(drd2_train_undersampled)), npool, replace=False)\n",
    "pool = drd2_train_undersampled.iloc[idxpool]\n",
    "drd2_train_undersampled.drop(idxpool, axis=0, inplace=True)\n",
    "d = 2048\n",
    "\n",
    "npts_more_human = 0\n",
    "keep_pool = np.random.choice(pool.index, npts_more_human, replace=False)\n",
    "drd2_train_undersampled_h = pd.concat((drd2_train_undersampled, pool.loc[keep_pool]))\n",
    "\n",
    "print(f\"Train size: {drd2_train.shape}\")\n",
    "print(f\"Train undersampled size: {drd2_train_undersampled.shape}\")\n",
    "print(f\"Train undersampled human size: {drd2_train_undersampled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "drd2_train_undersampled[\"activity_y\"] = drd2_train_undersampled.activity.values.tolist()\n",
    "drd2_train_undersampled_h[\"activity_h\"] = drd2_train_undersampled_h.activity.values.tolist()\n",
    "\n",
    "drd2_test[\"activity_y\"] = drd2_test.activity.values.tolist()\n",
    "drd2_test[\"activity_h\"] = drd2_test.activity.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size: torch.Size([930, 2048])\n"
     ]
    }
   ],
   "source": [
    "train_features = drd2_train_undersampled[[f\"bit{i}\" for i in range(d)]].values\n",
    "train_features_h = drd2_train_undersampled_h[[f\"bit{i}\" for i in range(d)]].values\n",
    "train_labels = drd2_train_undersampled[[\"activity_y\"]].values\n",
    "train_labels_h = drd2_train_undersampled_h[[\"activity_h\"]].values\n",
    "\n",
    "test_features = drd2_test[[f\"bit{i}\" for i in range(d)]].values\n",
    "test_labels = drd2_test[[\"activity_y\", \"activity_h\"]].values\n",
    "\n",
    "X_train = torch.tensor(train_features, dtype=torch.double)\n",
    "X_train_h = torch.tensor(train_features_h, dtype=torch.double)\n",
    "y_train = torch.tensor(train_labels, dtype=torch.double)\n",
    "h_train = torch.tensor(train_labels_h, dtype=torch.double)\n",
    "\n",
    "X_test = torch.tensor(test_features, dtype=torch.double)\n",
    "y_test = torch.tensor(test_labels, dtype=torch.double)\n",
    "\n",
    "idx_active = torch.where(y_test[:, 0])[0].numpy()\n",
    "idx_inactive = [i for i in range(len(y_test)) if i not in idx_active]\n",
    "idx_inactive = np.random.choice(range(len(y_test)), 600, replace=False)\n",
    "idx = np.r_[idx_active,idx_inactive]\n",
    "y_test = y_test[idx]\n",
    "X_test = X_test[idx]\n",
    "print(f\"Test size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example usage:\n",
    "num_features = train_features.shape[1]  # number of input features\n",
    "dropout = 0.2\n",
    "num_epochs = 200\n",
    "lr = 0.1\n",
    "\n",
    "# create an instance of the NonLinearNetDefer\n",
    "l2d_model = NonLinearNetDefer(num_features, dropout)\n",
    "\n",
    "# define the loss function and optimizer for the l2d_model\n",
    "criterion = nn.BCEWithLogitsLoss()  # use BCEWithLogitsLoss for binary classification\n",
    "optimizer = optim.SGD(l2d_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train\n",
    "X_h = X_train_h\n",
    "\n",
    "# binary labels for classifier 1 and classifier 2 (or human model)\n",
    "y_train = y_train[:,0].unsqueeze(1)\n",
    "h_train = h_train[:,0].unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLUSTERING:\n",
    "    algo = KMeans(n_clusters=2, random_state=seed, n_init=20, max_iter=5000, init=\"k-means++\")\n",
    "    algo.fit(X)\n",
    "    c1 = np.where(algo.labels_)[0]\n",
    "    c2 = [i for i in range(len(X_train)) if i not in c1]\n",
    "\n",
    "    X_train = X_train[c1, :]\n",
    "    X_train_h = X_train_h[c2,:]\n",
    "\n",
    "    y_train = y_train[c1,0].unsqueeze(1)\n",
    "    h_train = h_train[c2,0].unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add random noise to h\n",
    "# oldh = torch.clone(h)\n",
    "# p =torch.bernoulli(0.75 *torch.ones(len(h))).unsqueeze(1)\n",
    "# h = p*h + (1-p)*(1-h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200], Loss: 2.5956365553312883\n",
      "Epoch [20/200], Loss: 2.5187038253311966\n",
      "Epoch [30/200], Loss: 2.455695939768825\n",
      "Epoch [40/200], Loss: 2.404498006713122\n",
      "Epoch [50/200], Loss: 2.3712669032138787\n",
      "Epoch [60/200], Loss: 2.351223642155281\n",
      "Epoch [70/200], Loss: 2.3365356985125785\n",
      "Epoch [80/200], Loss: 2.3223381665877847\n",
      "Epoch [90/200], Loss: 2.317269460886917\n",
      "Epoch [100/200], Loss: 2.307540491607222\n",
      "Epoch [110/200], Loss: 2.3063412124676033\n",
      "Epoch [120/200], Loss: 2.3003223968840727\n",
      "Epoch [130/200], Loss: 2.296475956895193\n",
      "Epoch [140/200], Loss: 2.2914843964024927\n",
      "Epoch [150/200], Loss: 2.29005039541522\n",
      "Epoch [160/200], Loss: 2.2890985082053894\n",
      "Epoch [170/200], Loss: 2.2851762731533647\n",
      "Epoch [180/200], Loss: 2.282904700500135\n",
      "Epoch [190/200], Loss: 2.2810526653570777\n",
      "Epoch [200/200], Loss: 2.276876294013864\n"
     ]
    }
   ],
   "source": [
    "optimization_loop(num_epochs, optimizer, l2d_model, X_train, X_train_h, y_train, h_train, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"clf_1\": {\n",
      "    \"Accuracy\": 0.9344086021505377,\n",
      "    \"Precision\": 0.9432180598525441,\n",
      "    \"Recall\": 0.9344086021505377,\n",
      "    \"F1-Score\": 0.9350351898406438\n",
      "  },\n",
      "  \"clf_2\": {\n",
      "    \"Accuracy\": 0.6053763440860215,\n",
      "    \"Precision\": 0.3664805179789571,\n",
      "    \"Recall\": 0.6053763440860215,\n",
      "    \"F1-Score\": 0.4565664858947489\n",
      "  },\n",
      "  \"system\": {\n",
      "    \"Accuracy\": 0.946236559139785,\n",
      "    \"Precision\": 0.9522303935441108,\n",
      "    \"Recall\": 0.946236559139785,\n",
      "    \"F1-Score\": 0.946689628292985\n",
      "  }\n",
      "}\n",
      "Percentage of deferral: 0.5537634491920471\n",
      "Deferral for positive samples: 0.00272479560226202\n",
      "Deferral for negative samples: 0.912966251373291\n"
     ]
    }
   ],
   "source": [
    "y_test = y_test[:,0].unsqueeze(1)\n",
    "h_test = y_test[:,0].unsqueeze(1) # y = h\n",
    "metrics = {}\n",
    "labels = [y_test, h_test]\n",
    "\n",
    "with torch.no_grad():\n",
    "    l2d_model.eval()\n",
    "    combined_outputs, decision_outputs = l2d_model(X_test)\n",
    "    pred_clf = (combined_outputs > 0.5).float()\n",
    "    for i in range(2):\n",
    "        metrics[f\"clf_{i+1}\"] = get_metrics(labels[i], pred_clf[:, i])\n",
    "    boolean = (\n",
    "            decision_outputs[:, 0] > combined_outputs[:, 0]\n",
    "        ) * (\n",
    "            combined_outputs[:, 1] > combined_outputs[:, 0]\n",
    "        ) + (\n",
    "            decision_outputs[:,0] > combined_outputs[:, 0]\n",
    "        ) * (\n",
    "            combined_outputs[:, 1] < combined_outputs[:, 0]\n",
    "        ) * 1.\n",
    "    boolean = torch.tensor(boolean, dtype=torch.float32)\n",
    "\n",
    "    final_predictions = (boolean * pred_clf[:, 1]) + (1 - boolean) * pred_clf[:, 0]\n",
    "    metrics[f\"system\"] = get_metrics(labels[0], final_predictions)\n",
    "print (json.dumps(metrics, indent=2, default=str))\n",
    "print(f\"Percentage of deferral: {boolean.mean()}\")\n",
    "print(f\"Deferral for positive samples: {boolean[(labels[0] == 1).squeeze()].mean()}\")\n",
    "print(f\"Deferral for negative samples: {boolean[(labels[0] == 0).squeeze()].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_test)):\n",
    "    if (y_test[i]==1) and (decision_outputs[i] > combined_outputs[i, 0]) and (combined_outputs[i, 1] > combined_outputs[i, 0]) or (y_test[i]==0) and (decision_outputs[i] > combined_outputs[i, 0]) and (combined_outputs[i, 1] < combined_outputs[i, 0]):\n",
    "        print(\n",
    "            f\"y = {y_test[i].item()},\", \n",
    "            f\"h = {h_test[i].item()},\", \n",
    "            f\"classifier pred = {combined_outputs[i,0]:.3f},\", \n",
    "            f\"hum model pred = {combined_outputs[i,1]:.3f}\",\n",
    "            \"--> deferred\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"y = {y_test[i].item()},\", \n",
    "            f\"h = {h_test[i].item()},\", \n",
    "            f\"classifier pred = {combined_outputs[i,0]:.3f},\", \n",
    "            f\"hum model pred = {combined_outputs[i,1]:.3f}\",\n",
    "            \"--> NOT deferred\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(l2d_model.state_dict(), \"models/l2d_model_demo2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drd2_train_undersampled.iloc[c1].rename(columns = {\"activity\": \"activity_y\"}).to_csv(\"datasets/drd2_train_undersampled_y_ECFP_counts.csv\")\n",
    "drd2_train_undersampled.iloc[c2].rename(columns = {\"activity\": \"activity_h\"}).to_csv(\"datasets/drd2_train_undersampled_h_ECFP_counts.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "L2D",
   "language": "python",
   "name": "l2d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
